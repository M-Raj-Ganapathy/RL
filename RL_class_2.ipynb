{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "REINFORCEMENT LEARNING - REG NO:RA2412049015036 - DATE:22.02.2025"
      ],
      "metadata": {
        "id": "ZBF1jgsqYagW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaBIXnZ6Bnvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 4- Cart Pole Balancing with Random Policy, Unified Notation for Episodic and\n",
        "# Continuing Tasks, Policies and Value Functions, Optimal Policies and Optimal Value Functions, Optimality and Approximation"
      ],
      "metadata": {
        "id": "JYckhMHSBn05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "def run_cartpole_random_policy(episodes=10, seed=42):\n",
        "    # Create CartPole environment with a seed for reproducibility\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # Use \"human\" for rendering\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, info = env.reset(seed=seed + episode)  # Reset the environment with a unique seed\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            env.render()  # Render the environment (optional, can slow execution)\n",
        "            action = env.action_space.sample()  # Select a random action (0 or 1)\n",
        "            state, reward, terminated, truncated, info = env.step(action)  # Take the action\n",
        "            done = terminated or truncated  # Check if the episode is done\n",
        "            total_reward += reward\n",
        "\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_cartpole_random_policy(episodes=5)  # Run 5 episodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqz10PPn-yxV",
        "outputId": "a5f83236-2d84-45ff-a751-17aba049d6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = 18.0\n",
            "Episode 2: Total Reward = 10.0\n",
            "Episode 3: Total Reward = 14.0\n",
            "Episode 4: Total Reward = 9.0\n",
            "Episode 5: Total Reward = 18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AesYt0WJ-yz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 5- Policy Iteration on GridWorld problem, Dynamic Programming-\n",
        "# Policy Evaluation (Prediction), Policy Improvement, Policy Iteration,\n",
        "# Value Iteration, Asynchronous Dynamic Programming, Generalized Policy Iteration,"
      ],
      "metadata": {
        "id": "UJyTylzsBs86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the GridWorld environment\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size=(4, 4), goal=(3, 3)):\n",
        "        self.grid_size = grid_size\n",
        "        self.goal = goal\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.states = [(i, j) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
        "        self.rewards = {s: -1 for s in self.states}  # Default reward\n",
        "        self.rewards[goal] = 0  # Goal state has reward 0\n",
        "        self.terminals = [goal]  # Terminal state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminals:\n",
        "            return state, 0, True  # Terminal state, no further action\n",
        "\n",
        "        next_state = list(state)\n",
        "        if action == 'up':\n",
        "            next_state[0] = max(0, next_state[0] - 1)\n",
        "        elif action == 'down':\n",
        "            next_state[0] = min(self.grid_size[0] - 1, next_state[0] + 1)\n",
        "        elif action == 'left':\n",
        "            next_state[1] = max(0, next_state[1] - 1)\n",
        "        elif action == 'right':\n",
        "            next_state[1] = min(self.grid_size[1] - 1, next_state[1] + 1)\n",
        "\n",
        "        next_state = tuple(next_state)\n",
        "        reward = self.rewards[next_state]\n",
        "        done = next_state in self.terminals\n",
        "        return next_state, reward, done\n",
        "\n",
        "# Policy Iteration Algorithm\n",
        "def policy_iteration(grid, gamma=0.9, theta=1e-6):\n",
        "    states = grid.states\n",
        "    actions = grid.actions\n",
        "    policy = {s: np.random.choice(actions) for s in states}  # Random initial policy\n",
        "    V = {s: 0 for s in states}  # Initialize value function\n",
        "\n",
        "    while True:\n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in states:\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "                next_state, reward, _ = grid.step(s, a)\n",
        "                V[s] = reward + gamma * V[next_state]\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        for s in states:\n",
        "            old_action = policy[s]\n",
        "            action_values = {}\n",
        "            for a in actions:\n",
        "                next_state, reward, _ = grid.step(s, a)\n",
        "                action_values[a] = reward + gamma * V[next_state]\n",
        "            policy[s] = max(action_values, key=action_values.get)\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "# Run Policy Iteration\n",
        "grid = GridWorld()\n",
        "policy, V = policy_iteration(grid)\n",
        "print(\"Optimal Policy:\", policy)\n",
        "print(\"Value Function:\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmA_DRDvBs_U",
        "outputId": "3c5ca78b-c267-414b-c93d-0d1787724398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: {(0, 0): 'down', (0, 1): 'down', (0, 2): 'down', (0, 3): 'down', (1, 0): 'down', (1, 1): 'down', (1, 2): 'down', (1, 3): 'down', (2, 0): 'down', (2, 1): 'down', (2, 2): 'down', (2, 3): 'down', (3, 0): 'right', (3, 1): 'right', (3, 2): 'right', (3, 3): 'up'}\n",
            "Value Function: {(0, 0): -4.0951, (0, 1): -3.439, (0, 2): -2.71, (0, 3): -1.9, (1, 0): -3.439, (1, 1): -2.71, (1, 2): -1.9, (1, 3): -1.0, (2, 0): -2.71, (2, 1): -1.9, (2, 2): -1.0, (2, 3): 0.0, (3, 0): -1.9, (3, 1): -1.0, (3, 2): 0.0, (3, 3): 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAwm-Fs8CnVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 6- Value iteration on gamblerâ€™s problem."
      ],
      "metadata": {
        "id": "ICVIjENeCnkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration_gamblers(goal=100, p_win=0.4, gamma=1.0, theta=1e-6):\n",
        "    states = range(goal + 1)\n",
        "    V = np.zeros(goal + 1)  # Value function\n",
        "    policy = np.zeros(goal + 1)  # Policy\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == 0 or s == goal:\n",
        "                continue  # Terminal states\n",
        "            v = V[s]\n",
        "            action_values = []\n",
        "            for a in range(1, min(s, goal - s) + 1):  # Possible bets\n",
        "                win_state = s + a\n",
        "                lose_state = s - a\n",
        "                action_value = p_win * (gamma * V[win_state]) + (1 - p_win) * (gamma * V[lose_state])\n",
        "                action_values.append(action_value)\n",
        "            V[s] = max(action_values)\n",
        "            policy[s] = np.argmax(action_values) + 1  # Best action\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "# Run Value Iteration\n",
        "policy, V = value_iteration_gamblers()\n",
        "print(\"Optimal Policy:\", policy)\n",
        "print(\"Value Function:\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSiYmyNRBtBb",
        "outputId": "ee2ad4af-ae72-425b-bdd5-ab4cb0574fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0.]\n",
            "Value Function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taEMG6DOBtFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBSRO6wZBtH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bajTLsDBtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwyNclqqBtNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}